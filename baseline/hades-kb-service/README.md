# hades-kb-service

knowledge base service for gate's agent

## Tech Stack

- [Poetry](https://python-poetry.org/docs) - Python Package Manager [[Learn]](https://python-poetry.org/docs/basic-usage/)
- [Gunicorn](https://gunicorn.org) - Python WSGI HTTP Server
- [FastAPI](https://fastapi.tiangolo.com) - Python Web Framework [[Learn]](https://fastapi.tiangolo.com/tutorial/)
- [LangChain](https://www.langchain.com/langchain) - Framework for developing LLM applications [[Learn]](https://python.langchain.com/v0.2/docs/tutorials/)
- [LangServe](https://www.langchain.com/langserve) - Batteries included library for serving LLM executables via FastAPI [[Learn]](https://python.langchain.com/v0.2/docs/langserve)
- [LangSmith](https://www.langchain.com/langsmith) - LLM Tracing and Evaluation Tool [[Learn]](https://docs.smith.langchain.com/tutorials)
- [Langsmith Evaluation](https://docs.smith.langchain.com/old/evaluation) - Langsmith Tool to create Dataset and Examples to Test LLM Agent [Helix Techdocs](https://helix.engtools.net/catalog/default/component/integrating_llm_apps_with_grabgpt_developer_guide/docs/how-to-evaluate)

Read [LangChain stacks introduction](https://python.langchain.com/v0.2/docs/introduction) to understand the LLM app development.

## Prerequisites

- Python 3.x
- Poetry

## Quick Start

```sh
# Setup tools needed and pre-commit hooks
$ make setup

# Entering the Python virtual environment mode
$ poetry shell

# Install Python dependencies
$ poetry install

# Create a gitignored secret.ini, and fill in the required secrets for development
$ cp configs/secret.ini.example configs/secret.ini

# Run the application with hot reload
$ make dev

# Validate if the LLM agent is setting up correctly

## Option 1: LangServe Playground
## Navigate to the playground UI generated by LangServe and fill up the form
$ open http://localhost:8088/agent/simple/playground

## Option 2: cURL
## Invoke the agent API with a sample input
$ curl -X POST "http://localhost:8088/agent/simple/invoke" -H "Content-Type: application/json" -d '{"input": {"input": "Hello world", "chat_history": []}}'

# If you have configured the LangSmith (LangChain) token in the `secret.ini`, you may check the LLM agent's execution trace on LangSmith after the invocation of the agent API
## For local development and staging environment
$ open https://langsmith.stg.cauldron.myteksi.net/projects

## For production environment
$ open https://langsmith.cauldron.myteksi.net/projects
```

## Setup local postgresql database

### Start postgresql in your local

```shell
$ brew install postgresql

$ brew install pgvector

$ brew services start postgresql
```

### Database migrations

The setup is identical to Grab-kit's database migrations. You can run the following commands to create and run migrations.

```shell
# One time setup
$ ./scripts/db.sh --create

# Run migrations
$ ./scripts/db.sh --up
```

## Useful commands

```sh
# Setup pre-commit hooks for linting and formatting checks
$ make pre-commit

# Run linting, formatting checks and auto-fixes
$ make format

# Run all unit tests with coverage report
$ make test

# Checkout all available commands
$ make help
```

## API Documentation

> Swagger <http://localhost:8088/docs>
> Redoc <http://localhost:8088/redoc>

## How to start modifying your LLM Agent

To understand how to utlize this agent framework, you can start from /app/agents/simple_agent.py. You can start by adding tools in the tools variable and modifying the prompt. Tools are essentially functions, you can call your APIs, extract information from database in those functions or do anything you want.

## Project structure

    ├── app                       - Application source code
    │   ├── agents                - LLM agents
    │   ├── core                  - Application configuration, startup events, logging.
    │   ├── models                - LLM models
    │   ├── routes                - API endpoints and request handlers.
    │   ├── storage               - Storage handlers including connection and table models  
    │   ├── server.py             - FastAPI application creation and configuration.
    │   └── tools                 - Tools for LLM Agent
    ├── configs                   - Configuration files for the application.
    ├── databases                 - Database related scripts
    ├── scripts                   - Scripts used for various purposes.
    ├── tests                     - Testing files
    │   └── langsmith_evaluation  - Dataset Example using Langsmith Evaluation on LLM Agent
    │   └── unit_tests            - Unit tests - pytest
    ├── Dockerfile                - Dockerfile for building the image
    ├── .pre-commit-config.yaml   - Configuration file for pre-commit git hooks
    └── Makefile                  - Makefile for common commands

## Debugging

### With VSCode

1. Install the Python extension for VSCode.
   <https://marketplace.visualstudio.com/items?itemName=ms-python.debugpy>

2. Add the following configuration to `.vscode/launch.json`:

```json
{
  "version": "0.2.0",
  "configurations": [
      {
          "name": "Debug App",
          "type": "debugpy",
          "request": "launch",
          "module": "uvicorn",
          "args": [
              "app.server:app",
              "--host",
              "0.0.0.0",
              "--port",
              "8088",
              "--reload"
          ],
          "env": {}
      }
  ]
}
```

Generated by [LLM App Cookiecutter template](https://gitlab.myteksi.net/techops-automation/gate/llm-app-template.git) on 2024-07-02.
