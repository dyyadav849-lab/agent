# Comprehensive Technical Documentation: TI-Bot Baseline Pipeline Services

## Executive Summary

This document provides a complete technical analysis of the TI-Bot Baseline Pipeline consisting of two primary microservices:
1. **hades-kb-service**: Knowledge Base service for document and Slack RAG (Retrieval-Augmented Generation)
2. **Zion**: LLM Agents service with multi-agent orchestration capabilities

Both services implement traditional AI/ML pipelines with basic vector storage, simple chunking, and standard monitoring.

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                TI-Bot Baseline Pipeline Architecture            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                 â”‚         â”‚                                 â”‚ â”‚
â”‚  â”‚  Zion Service   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤  hades-kb-service              â”‚ â”‚
â”‚  â”‚  (Port 8000)    â”‚         â”‚  (Port 8088)                   â”‚ â”‚
â”‚  â”‚                 â”‚         â”‚                                 â”‚ â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚ â”‚
â”‚  â”‚ â”‚Basic Agent  â”‚ â”‚         â”‚ â”‚Basic RAG    â”‚                 â”‚ â”‚
â”‚  â”‚ â”‚System       â”‚ â”‚         â”‚ â”‚Processor    â”‚                 â”‚ â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚ â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚ â”‚
â”‚  â”‚ â”‚Simple Tool  â”‚ â”‚         â”‚ â”‚Simple Slack â”‚                 â”‚ â”‚
â”‚  â”‚ â”‚Handlers     â”‚ â”‚         â”‚ â”‚Processor    â”‚                 â”‚ â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚ â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚ â”‚
â”‚  â”‚ â”‚Standard     â”‚ â”‚         â”‚ â”‚Vector Store â”‚                 â”‚ â”‚
â”‚  â”‚ â”‚Workflows    â”‚ â”‚         â”‚ â”‚(pgvector)   â”‚                 â”‚ â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                              â”‚                       â”‚
â”‚           â”‚                              â”‚                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                 â”‚         â”‚                                 â”‚ â”‚
â”‚  â”‚ External APIs   â”‚         â”‚ Databases & Storage             â”‚ â”‚
â”‚  â”‚                 â”‚         â”‚                                 â”‚ â”‚
â”‚  â”‚ â€¢ OpenAI        â”‚         â”‚ â€¢ PostgreSQL + pgvector        â”‚ â”‚
â”‚  â”‚ â€¢ Confluence    â”‚         â”‚ â€¢ MySQL                         â”‚ â”‚
â”‚  â”‚ â€¢ Jira          â”‚         â”‚ â€¢ Basic Storage                 â”‚ â”‚
â”‚  â”‚ â€¢ GitLab        â”‚         â”‚                                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Service 1: hades-kb-service (Knowledge Base)

### Directory Structure
```
hades-kb-service/
â”œâ”€â”€ app/                              # Main application code
â”‚   â”œâ”€â”€ auth/                         # Authentication & authorization
â”‚   â”‚   â”œâ”€â”€ bearer.py                 # Bearer token handling
â”‚   â”‚   â”œâ”€â”€ modes.py                  # Auth modes (proxy, session)
â”‚   â”‚   â”œâ”€â”€ oidc.py                   # OpenID Connect integration
â”‚   â”‚   â””â”€â”€ sessions/                 # Session management
â”‚   â”œâ”€â”€ core/                         # Core business logic
â”‚   â”‚   â”œâ”€â”€ azure_em/                 # Azure Embedding Models
â”‚   â”‚   â”œâ”€â”€ config.py                 # Configuration management
â”‚   â”‚   â”œâ”€â”€ dependencies.py           # Dependency injection
â”‚   â”‚   â”œâ”€â”€ log/                      # Logging infrastructure
â”‚   â”‚   â”œâ”€â”€ ragdocument/              # RAG Document processing
â”‚   â”‚   â”‚   â””â”€â”€ client.py            # Main RAG client implementation
â”‚   â”‚   â”œâ”€â”€ ragslack/                 # RAG Slack processing
â”‚   â”‚   â”œâ”€â”€ s3/                       # S3 storage integration
â”‚   â”‚   â””â”€â”€ transformer/              # Text processing & chunking
â”‚   â”‚       â”œâ”€â”€ client.py            # Text transformation client
â”‚   â”‚       â””â”€â”€ text_splitter/       # Chunking strategies
â”‚   â”‚           â”œâ”€â”€ client.py        # Text splitting implementation
â”‚   â”‚           â””â”€â”€ models.py        # Splitter configuration
â”‚   â”œâ”€â”€ models/                       # Data models
â”‚   â”‚   â”œâ”€â”€ azure_openai_model.py    # Azure OpenAI model definitions
â”‚   â”‚   â””â”€â”€ utils.py                 # Model utilities
â”‚   â”œâ”€â”€ routes/                       # API route handlers
â”‚   â”‚   â”œâ”€â”€ api.py                   # Main API router
â”‚   â”‚   â”œâ”€â”€ doc_kb_route/            # Document KB endpoints
â”‚   â”‚   â”œâ”€â”€ health_check.py          # Health check endpoint
â”‚   â”‚   â”œâ”€â”€ oidc.py                  # OIDC authentication routes
â”‚   â”‚   â”œâ”€â”€ s3_route/                # S3 storage routes
â”‚   â”‚   â””â”€â”€ slack_kb_route/          # Slack KB endpoints
â”‚   â”œâ”€â”€ server.py                    # FastAPI application setup
â”‚   â”œâ”€â”€ storage/                     # Database & storage layer
â”‚   â”‚   â”œâ”€â”€ connection.py            # Database connection management
â”‚   â”‚   â”œâ”€â”€ ragdocument_db/          # Document storage
â”‚   â”‚   â”‚   â”œâ”€â”€ client.py           # Database client
â”‚   â”‚   â”‚   â”œâ”€â”€ models.py           # SQLAlchemy models
â”‚   â”‚   â”‚   â””â”€â”€ constant.py         # Database constants
â”‚   â”‚   â””â”€â”€ ragslack_db/             # Slack storage
â”‚   â”œâ”€â”€ tools/                       # LLM tools and utilities
â”‚   â”œâ”€â”€ tracing/                     # OpenTelemetry tracing
â”‚   â”‚   â””â”€â”€ tracer.py               # Trace configuration
â”‚   â””â”€â”€ utils/                       # Utility functions
â”œâ”€â”€ configs/                         # Configuration files
â”‚   â”œâ”€â”€ dev.ini                     # Development config
â”‚   â”œâ”€â”€ prd.ini                     # Production config
â”‚   â”œâ”€â”€ stg.ini                     # Staging config
â”‚   â””â”€â”€ secret.ini.example          # Secret configuration template
â”œâ”€â”€ databases/                       # Database migrations
â”œâ”€â”€ scripts/                         # Utility scripts
â””â”€â”€ tests/                          # Test suites
```

### ğŸ”§ RAG Implementation Details

#### 1. **Chunking Strategies** (`app/core/transformer/`)

**File**: `app/core/transformer/text_splitter/client.py`

```python
class TextSplitterClient:
    def split_text(self, text: str, chunk_size: int, chunk_overlap: int = 0, splitter_selector: int = 0):
        """
        Available Chunking Strategies:
        - DEFAULT (0): CharacterTextSplitter with tiktoken encoding
        - RECURSIVE (1): RecursiveCharacterTextSplitter with tiktoken encoding
        """
```

**Chunking Configuration**:
- **Default Chunk Size**: 512 tokens
- **Default Overlap**: 200 tokens
- **Encoding**: `cl100k_base` (GPT-4 tokenizer)
- **Strategy**: Basic character-based splitting

#### 2. **Vector Storage** (`app/storage/ragdocument_db/`)

**Database Schema** (`models.py`):
```python
class DocumentEmbedding(Base):
    __tablename__ = "document_embedding"
    id = Column(BigInteger, primary_key=True)
    token_number = Column(BigInteger, nullable=False)
    embedding = Column(Vector(1536), nullable=False)  # OpenAI ada-002 dimensions
    document_information_id = Column(BigInteger, ForeignKey("document_information.id"))
    text_snipplet = Column(Text, nullable=False)
    status = Column(Text, nullable=False, default=ACTIVE_STATUS)
```

**Vector Search Implementation** (`client.py`):
```python
def read_document_embedding_data(self, embeded_query: List[float], document_collection_uuids: List[str], 
                                embedding_operator: str = "<#>", vector_threshold: float = 0.7):
    """
    Vector similarity search using pgvector
    Standard operators: <#> (negative inner product), <-> (L2 distance), <=> (cosine distance)
    """
```

#### 3. **Embedding Models** (`app/core/azure_em/`)

**Default Model**: `text-embedding-ada-002`
**Supported Models**: Azure OpenAI embedding models
**Timeout**: 300 seconds (configurable)

#### 4. **Document Processing Pipeline**

**Flow**:
1. **Document Ingestion** â†’ **Basic Text Extraction** â†’ **Simple Preprocessing** â†’ **Basic Chunking** â†’ **Embedding** â†’ **Vector Storage**

**Text Preprocessing** (`ragdocument/client.py`):
```python
def text_pre_processing(self, text: str, chunk_size: int = 512, chunk_overlap: int = 200, splitter_selector: int = 1):
    text = text.lower()  # Convert to lowercase
    # Basic chunk validation
    # Standard chunking via transformer client
```

---

## ğŸ¤– Service 2: Zion (LLM Agents Service)

### Directory Structure
```
zion/
â”œâ”€â”€ zion/                            # Main application code
â”‚   â”œâ”€â”€ agent/                       # Agent implementations
â”‚   â”‚   â”œâ”€â”€ agent_builder.py         # Agent construction logic
â”‚   â”‚   â”œâ”€â”€ model.py                 # LLM model abstractions
â”‚   â”‚   â”œâ”€â”€ multi_agent/             # Multi-agent workflows
â”‚   â”‚   â”‚   â”œâ”€â”€ multi_agent_workflow.py  # Basic workflows
â”‚   â”‚   â”‚   â”œâ”€â”€ able_to_answer_agent.py  # Answer validation agent
â”‚   â”‚   â”‚   â”œâ”€â”€ internal_search_agent.py # Internal search agent
â”‚   â”‚   â”‚   â”œâ”€â”€ query_categorizer_agent.py # Query classification
â”‚   â”‚   â”‚   â””â”€â”€ ti_bot_agent.py      # Main TI-Bot agent
â”‚   â”‚   â”œâ”€â”€ single_agent/            # Single agent workflows
â”‚   â”‚   â”œâ”€â”€ react_agent_builder.py   # ReAct agent implementation
â”‚   â”‚   â””â”€â”€ zion_agent.py           # Main agent orchestrator
â”‚   â”œâ”€â”€ config.py                    # Configuration management
â”‚   â”œâ”€â”€ credentials/                 # Authentication credentials
â”‚   â”œâ”€â”€ data/                        # Data models and schemas
â”‚   â”œâ”€â”€ evaluations/                 # Basic evaluation framework
â”‚   â”œâ”€â”€ jobs/                        # Background job processing
â”‚   â”œâ”€â”€ main.py                      # FastAPI application
â”‚   â”œâ”€â”€ stats/                       # Metrics and monitoring
â”‚   â”‚   â””â”€â”€ datadog.py              # Datadog integration
â”‚   â”œâ”€â”€ tool/                        # Agent tools and capabilities
â”‚   â”‚   â”œâ”€â”€ hades_kb_service.py     # Integration with hades-kb-service
â”‚   â”‚   â”œâ”€â”€ glean_search.py         # Enterprise search
â”‚   â”‚   â”œâ”€â”€ gitlab_job_trace_tool.py # GitLab integration
â”‚   â”‚   â”œâ”€â”€ jira_jql_search_tool.py # Jira integration
â”‚   â”‚   â”œâ”€â”€ kibana_search_tool.py   # Log search
â”‚   â”‚   â””â”€â”€ [40+ other tools]       # Standard tool ecosystem
â”‚   â”œâ”€â”€ tracing/                     # OpenTelemetry tracing
â”‚   â””â”€â”€ util/                        # Utility functions
â”œâ”€â”€ configs/                         # Configuration files
â”œâ”€â”€ docs/                           # Documentation
â”œâ”€â”€ playground/                      # Development playground
â””â”€â”€ tests/                          # Test suites
```

### ğŸ§  Agent Architecture

#### **Agent Types Supported**:
1. **`react_agent`**: Basic ReAct (Reasoning + Acting) pattern
2. **`multi_agent`**: Simple multi-agent workflows
3. **`agent_executor`**: Standard LangChain agent executor

#### **Multi-Agent Workflow** (`multi_agent/multi_agent_workflow.py`):

```python
def get_ti_bot_multi_agent_system(tools, model, prompts, descriptions):
    workflow = StateGraph(AgentState)
    
    # Basic Agent Flow
    workflow.add_node("query_categorizer_agent", create_query_categorizer_agent_node(...))
    workflow.add_node("ti_bot_agent", create_ti_bot_agent_node(...))
    workflow.add_node("internal_search_agent", create_internal_search_agent_node(...))
    workflow.add_node("able_to_answer_agent", create_able_to_answer_agent_node(...))
```

**Basic Workflow Flow**:
```
Query â†’ Simple Categorizer â†’ TI-Bot Agent â†’ Basic Search â†’ Answer Validation â†’ Response
```

### ğŸ› ï¸ Tool Ecosystem (40+ Tools)

#### **Knowledge & Search Tools**:
- **`HadesKnowledgeBaseTool`**: Integration with hades-kb-service
- **`GleanSearchTool`**: Enterprise search
- **`KibanaSearchTool`**: Log and metrics search
- **`JiraJQLSearchTool`**: Issue tracking search

#### **Development Tools**:
- **`GitlabJobTraceTool`**: CI/CD pipeline investigation
- **`GitlabRepositoryAccessCheckerTool`**: Repository access verification
- **`SourcegraphTool`**: Code search

#### **Infrastructure Tools**:
- **`EC2LogRetrieverTool`**: AWS infrastructure logs
- **`GetServiceDependenciesTool`**: Service dependency mapping
- **`GetServicePlatformTool`**: Platform information

#### **Utility Tools**:
- **`CalculatorTool`**: Mathematical computations
- **`OpenAPITool`**: API documentation access
- **`UniversalSearchTool`**: Multi-source search

---

## ğŸ“Š Metrics & Monitoring

### **1. OpenTelemetry Tracing** (Both Services)

**Configuration** (`tracing/tracer.py`):
```python
otel_resource = Resource.create(attributes={
    SERVICE_NAME: "hades-kb-service" / "zion",
    SERVICE_VERSION: "1.0.0",
    KUBERNETES_NAMESPACE_NAME: os.environ.get(ENV_POD_NAMESPACE),
    KUBERNETES_POD_NAME: os.environ.get(ENV_POD_NAME),
    CONTAINER_IMAGE_NAME: container_image_name,
    CONTAINER_IMAGE_TAG: container_image_tag,
})
```

**Traced Components**:
- HTTP requests/responses
- Database queries
- LLM model calls
- Basic agent workflows
- Tool executions

### **2. Datadog Metrics** (Zion)

**Metric Categories** (`stats/datadog.py`):
```python
class DatadogClient:
    METRIC_SLACK = "slack"
    METRIC_AGENT = "agent" 
    METRIC_EXTERNAL = "external"
    EVENT_EVALUATION = "evaluation"
```

**Tracked Metrics**:
- Agent invocation counts
- Tool execution durations
- Model response times
- Error rates
- Basic evaluation scores

### **3. LangSmith Integration** (Both Services)

**Configuration**:
- **Endpoint**: `https://langsmith.stg.cauldron.myteksi.net/api`
- **Projects**: Per-agent project tracking
- **Traces**: Basic LLM conversation traces
- **Evaluations**: Standard testing datasets

---

## ğŸ”§ How to Run Both Services Together

### **Prerequisites**:
```bash
# Required software
- Python 3.11+
- Poetry
- PostgreSQL with pgvector
- MySQL

# Services
- Docker (for databases)
- Git
```

### **Service Pipeline Setup Guide**:

#### **Step 1: Environment Setup**
```bash
# Navigate to baseline services
cd /Users/deepak.yadav/dp_grab/getlabtest/ti-bot_deep/baseline

# Setup both services
cd hades-kb-service && poetry install && cd ..
cd zion && poetry install && cd ..
```

#### **Step 2: Database Setup**
```bash
# PostgreSQL with pgvector (for hades-kb-service)
brew install postgresql pgvector
brew services start postgresql

# MySQL (for Zion)
brew install mysql
brew services start mysql

# Create databases
createdb hades_kb_service
mysql -u root -e "CREATE DATABASE zion;"
```

#### **Step 3: Configuration**
```bash
# hades-kb-service secrets
cd hades-kb-service
cp configs/secret.ini.example configs/secret.ini
# Edit configs/secret.ini with your secrets

# Zion secrets  
cd ../zion
cp configs/secret.ini.example configs/secret.ini
# Edit configs/secret.ini with your secrets
```

#### **Step 4: Database Migrations**
```bash
# hades-kb-service migrations
cd hades-kb-service
./scripts/db.sh --create
./scripts/db.sh --up

# Zion migrations
cd ../zion  
./scripts/db.sh --create
./scripts/db.sh --up
```

#### **Step 5: Start Services**

**Terminal 1 - hades-kb-service**:
```bash
cd baseline/hades-kb-service
poetry run python -m uvicorn app.server:app --reload --port 8088
```

**Terminal 2 - Zion**:
```bash
cd baseline/zion
poetry run python -m uvicorn zion.main:app --reload --port 8000
```

#### **Step 6: Verify Services**

**hades-kb-service**:
- Health Check: http://localhost:8088/health_check
- API Docs: http://localhost:8088/docs
- ReDoc: http://localhost:8088/redoc

**Zion**:
- Health Check: http://localhost:8000/
- API Docs: http://localhost:8000/docs
- Agent Playground: http://localhost:8000/agent/{agent_name}/playground/

### **Integration Points**:

1. **Zion â†’ hades-kb-service**: 
   - Tool: `HadesKnowledgeBaseTool`
   - Endpoint: `/agent-plugin/{agent_name}`
   - Purpose: Basic knowledge base search for agent responses

2. **Shared Components**:
   - LangSmith tracing
   - OpenTelemetry monitoring
   - Standard authentication patterns

---

## ğŸ¯ Baseline System Characteristics

### **hades-kb-service**:

**Core Features**:
- Standard RAG pipeline
- Basic vector storage with pgvector
- Simple chunking strategies
- Standard API documentation
- Basic separation of concerns

**Limitations**:
- Basic text preprocessing
- Fixed embedding dimensions
- Limited chunking strategies
- Minimal error handling

### **Zion**:

**Core Features**:
- Standard agent architecture
- Basic tool ecosystem (40+ tools)
- Simple workflow orchestration
- Standard monitoring and evaluation
- Basic agent configuration

**Limitations**:
- Simple workflow patterns
- Basic configuration management
- Standard error handling
- Limited context awareness

---

## ğŸ“ˆ Baseline Metrics

### **Performance Metrics**:
1. **Response Times**: Basic agent invocation latencies
2. **Throughput**: Standard requests per second
3. **Error Rates**: Failed requests percentage
4. **Token Usage**: LLM token consumption

### **Quality Metrics**:
1. **Relevance Scores**: Basic RAG retrieval accuracy
2. **Answer Confidence**: Standard agent response confidence
3. **User Satisfaction**: Basic feedback scores
4. **Tool Success Rates**: Standard tool performance

### **Operational Metrics**:
1. **Database Performance**: Standard query execution times
2. **Vector Search**: Basic similarity search latencies
3. **Memory Usage**: Standard service resource consumption
4. **Cache Hit Rates**: Basic cache effectiveness

---

## ğŸš€ Current Status Summary

**Both baseline services are functional:**

- âœ… **hades-kb-service**: Operational on port 8088
- âœ… **Zion**: Functional on port 8000
- âœ… **Integration**: Basic HTTP API communication
- âœ… **Monitoring**: Standard OpenTelemetry and Datadog integration
- âœ… **Documentation**: Basic API documentation available

The baseline pipeline represents a standard AI/ML system with traditional capabilities for knowledge base processing and basic agent orchestration.
